\def\paperversiondraft{draft}
\def\paperversionnormal{normal}

% If the paper version is set to 'normal' mode keep it,
% otherwise set it to 'draft' mode.
\ifx\paperversion\paperversionnormal
\else
  \def\paperversion{draft}
\fi

\documentclass[review, anonymous, acmsmall, screen]{acmart}

\def\acmversionanonymous{anonymous}
\def\acmversionjournal{journal}
\def\acmversionnone{none}

\makeatletter
\if@ACM@anonymous
  \def\acmversion{anonymous}
\else
  \def\acmversion{journal}
\fi
\makeatother

\usepackage{colortbl}

% 'draftonly' environment
\usepackage{environ}
\ifx\paperversion\paperversiondraft
\newenvironment{draftonly}{}{}
\else
\NewEnviron{draftonly}{}
\fi

% Most PL conferences are edited by conference-publishing.com. Follow their
% advice to add the following packages.
%
% The first enables the use of UTF-8 as character encoding, which is the
% standard nowadays. The second ensures the use of font encodings that support
% accented characters etc. (Why should I use this?). The mictotype package
% enables certain features 'to­wards ty­po­graph­i­cal per­fec­tion
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{microtype}

\usepackage{xargs}
\usepackage{lipsum}
\usepackage{xparse}
\usepackage{xifthen, xstring}
\usepackage{xspace}
\usepackage{marginnote}
\usepackage{etoolbox}
\usepackage[acronym,shortcuts]{glossaries}
\usepackage{amsmath}
\usepackage{thmtools} % required for autoref to lemmas
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{hyphenat}
\usepackage[shortcuts]{extdash}

\input{tex/setup.tex}
\input{tex/acm.tex}

\usemintedstyle{colorful}

% Newer versions of minted require the 'customlexer' argument for custom lexers
% whereas older versions require the '-x' to be passed via the command line.
\makeatletter
\ifcsdef{MintedExecutable}
{
  % minted v3
  % \newminted[mlir]{tools/lexers/MLIRLexer.py:MLIRLexerOnlyOps}{mathescape}
  % \newminted[xdsl]{tools/lexers/MLIRLexer.py:MLIRLexer}{mathescape, style=murphy}
  % \newminted[lean4]{tools/lexers/Lean4Lexer.py:Lean4Lexer}{mathescape}
  \newminted[mlir]{llvm}{mathescape}
  \newminted[xdsl]{llvm}{mathescape, style=murphy}
  \newminted[lean4]{llvm}{mathescape}

}
{
  \ifcsdef{minted@optlistcl@quote}
  {
    \newminted[mlir]{llvm}{mathescape}
    \newminted[xdsl]{llvm}{mathescape, style=murphy}
    \newminted[lean4]{llvm}{mathescape}

    % \newminted[mlir]{tools/lexers/MLIRLexer.py:MLIRLexerOnlyOps}{customlexer, mathescape}
    % \newminted[xdsl]{tools/lexers/MLIRLexer.py:MLIRLexer}{customlexer, mathescape, style=murphy}
    % \newminted[lean4]{tools/lexers/Lean4Lexer.py:Lean4Lexer}{customlexer, mathescape}
  }
  {
    \newminted[mlir]{llvm}{mathescape}
    \newminted[xdsl]{llvm}{mathescape, style=murphy}
    \newminted[lean4]{llvm}{mathescape}

    % \newminted[mlir]{tools/lexers/MLIRLexer.py:MLIRLexerOnlyOps -x}{mathescape}
    % \newminted[xdsl]{tools/lexers/MLIRLexer.py:MLIRLexer -x}{mathescape, style=murphy}
    % \newminted[lean4]{tools/lexers/Lean4Lexer.py:Lean4Lexer -x}{mathescape}
  }
}
\makeatother

% We use the following color scheme
%
% This scheme is both print-friendly and colorblind safe for
% up to four colors (including the red tones makes it not
% colorblind safe any more)
%
% https://colorbrewer2.org/#type=qualitative&scheme=Paired&n=4

\definecolor{pairedNegOneLightGray}{HTML}{cacaca}
\definecolor{pairedNegTwoDarkGray}{HTML}{827b7b}
\definecolor{pairedOneLightBlue}{HTML}{a6cee3}
\definecolor{pairedTwoDarkBlue}{HTML}{1f78b4}
\definecolor{pairedThreeLightGreen}{HTML}{b2df8a}
\definecolor{pairedFourDarkGreen}{HTML}{33a02c}
\definecolor{pairedFiveLightRed}{HTML}{fb9a99}
\definecolor{pairedSixDarkRed}{HTML}{e31a1c}

\createtodoauthor{grosser}{pairedOneLightBlue}
\createtodoauthor{sarah}{pairedTwoDarkBlue}
\createtodoauthor{authorThree}{pairedThreeLightGreen}
\createtodoauthor{authorFour}{pairedFourDarkGreen}
\createtodoauthor{authorFive}{pairedFiveLightRed}
\createtodoauthor{authorSix}{pairedSixDarkRed}

\newacronym{ir}{IR}{Intermediate Representation}

\graphicspath{{./images/}}

% Define macros that are used in this paper
%
% We require all macros to end with a delimiter (by default {}) to enusure
% that LaTeX adds whitespace correctly.
\makeatletter
\newcommand\requiredelimiter[2][########]{%
  \ifdefined#2%
    \def\@temp{\def#2#1}%
    \expandafter\@temp\expandafter{#2}%
  \else
    \@latex@error{\noexpand#2undefined}\@ehc
  \fi
}
\@onlypreamble\requiredelimiter
\makeatother

\newcommand\newdelimitedcommand[2]{
\expandafter\newcommand\csname #1\endcsname{#2}
\expandafter\requiredelimiter
\csname #1 \endcsname
}

\newdelimitedcommand{toolname}{Tool}

\usepackage{booktabs}
\newcommand{\ra}[1]{\renewcommand{\arraystretch}{#1}}

\usepackage[verbose]{newunicodechar}
\newunicodechar{₁}{\ensuremath{_1}}
\newunicodechar{₂}{\ensuremath{_2}}
\newunicodechar{∀}{\ensuremath{\forall}}
\newunicodechar{α}{\ensuremath{\alpha}}
\newunicodechar{β}{\ensuremath{\beta}}

% \circled command to print a colored circle.
% \circled{1} pretty-prints "(1)"
% This is useful to refer to labels that are embedded within figures.
\DeclareRobustCommand{\circled}[2][]{%
    \ifthenelse{\isempty{#1}}%
        {\circledbase{pairedOneLightBlue}{#2}}%
        {\autoref{#1}: \hyperref[#1]{\circledbase{pairedOneLightBlue}{#2}}}%
}

% listings don't write "Listing" in autoref without this.
\providecommand*{\listingautorefname}{Listing}
\renewcommand{\sectionautorefname}{Section}
\renewcommand{\subsectionautorefname}{Section}
\renewcommand{\subsubsectionautorefname}{Section}

\begin{document}

\title{Certified Instruction Selection at Scale}
\subtitle{Validating LLVM against the Authoritative RISC-V Semantics using Certified BitBlasting}

%% Author information
%% Contents and number of authors suppressed with 'anonymous'.
%% Each author should be introduced by \author, followed by
%% \authornote (optional), \orcid (optional), \affiliation, and
%% \email.
%% An author may have multiple affiliations and/or emails; repeat the
%% appropriate command.
%% Many elements are not rendered, but should be provided for metadata
%% extraction tools.
\author{First1 Last1}
\authornote{with author1 note}          %% \authornote is optional;
                                      %% can be repeated if necessary
\orcid{nnnn-nnnn-nnnn-nnnn}             %% \orcid is optional
\affiliation{
  \position{Position1}
  \department{Department1}              %% \department is recommended
  \institution{Institution1}            %% \institution is required
  \streetaddress{Street1 Address1}
  \city{City1}
  \state{State1}
  \postcode{Post-Code1}
  \country{Country1}
}
\email{first1.last1@inst1.edu}          %% \email is recommended

\author{First2 Last2}
\authornote{with author2 note}          %% \authornote is optional;
                                      %% can be repeated if necessary
\orcid{nnnn-nnnn-nnnn-nnnn}             %% \orcid is optional
\affiliation{
  \position{Position2a}
  \department{Department2a}             %% \department is recommended
  \institution{Institution2a}           %% \institution is required
  \streetaddress{Street2a Address2a}
  \city{City2a}
  \state{State2a}
  \postcode{Post-Code2a}
  \country{Country2a}
}
\email{first2.last2@inst2a.com}         %% \email is recommended
\affiliation{
  \position{Position2b}
  \department{Department2b}             %% \department is recommended
  \institution{Institution2b}           %% \institution is required
  \streetaddress{Street3b Address2b}
  \city{City2b}
  \state{State2b}
  \postcode{Post-Code2b}
  \country{Country2b}
}
\email{first2.last2@inst2b.org}         %% \email is recommended


\begin{abstract}
% An abstract should consist of six main sentences:
%  1. Introduction. In one sentence, what’s the topic?
%  2. State the problem you tackle.
%  3. Summarize (in one sentence) why nobody else has adequately answered the research question yet.
%  4. Explain, in one sentence, how you tackled the research question.
%  5. In one sentence, how did you go about doing the research that follows from your big idea.
%  6. As a single sentence, what’s the key impact of your research?

% (http://www.easterbrook.ca/steve/2010/01/how-to-write-a-scientific-abstract-in-six-easy-steps/)
Modern compiler frameworks like LLVM translate high-level code into efficient machine code via a series
of transformation passes. Among these, instruction selection (ISel) is the semantically critical backend phase
that maps the first-class intermediate representations to target-specific ISA instructions. Optimizing
compilers enhance this phase with performance-improving transformations, but due to the lack of formal
guarantees such have historically introduced subtle miscompilation bugs. Verified compilers, e.g., 
CompCert, address this by proving the entire compiler correct, however they sacrifice optimization 
flexibility, development speed and remain limited to niche use cases disconnected from general-purpose compilation.
Moreover, such compiler verification projects often rely on SMT solvers and manually derived ISA 
models, shifting trust boundaries from the compiler to the large unverified codebase of the solver 
and the authors understanding of the ISA. As a step towards scalable trustworthy backend compilation
we present a prototype of a modular, extensible and verified instruction selection pipeline for LLVM
IR verified against authoritative ISA semantics relying on a minimal trusted code base. By re-implementing
LLVM's instruction selection patterns, we shed light onto an opaque and underexplored part of the 
infrastructure and by modeling ISel using a library of locally verified rewrites with straightforward
integration of new patterns, we enable a rapidly evolving and scalable infrastructure. Implemented
in an interactive theorem prover, our system leverages a novel bit-vector library and the first 
fully end-to-end verified bit-blaster integrated into a proof assistant to automate the proof burden.
We provide a proof of concept that a modular, proof-automated approach for instruction selection, 
verified against ratified ISA semantics, can bring formal
guarantees to such a compiler backend pass and remain connected to realistic production compilers. We aim 
for this work to serve as a 
foundation for scaling trustworthy, proof-automated formal verification using proof assistants 
into mainstream compiler backends of today and tomorrow.
\end{abstract}

% Only add ACM notes and keywords in camera ready version
% Drop citations and footnotes in draft and blind mode.
\ifx\acmversion\acmversionanonymous
\settopmatter{printacmref=false} % Removes citation information below abstract
\renewcommand\footnotetextcopyrightpermission[1]{} % removes footnote with conference information in first column
\fi
\ifx\acmversion\acmversionjournal
%% 2012 ACM Computing Classification System (CSS) concepts
%% Generate at 'http://dl.acm.org/ccs/ccs.cfm'.
\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10011007.10011006.10011008</concept_id>
<concept_desc>Software and its engineering~General programming languages</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10003456.10003457.10003521.10003525</concept_id>
<concept_desc>Social and professional topics~History of programming languages</concept_desc>
<concept_significance>300</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Software and its engineering~General programming languages}
\ccsdesc[300]{Social and professional topics~History of programming languages}
%% End of generated code

%% Keywords
%% comma separated list
\keywords{keyword1, keyword2, keyword3}
\fi

%% \maketitle
%% Note: \maketitle command must come after title commands, author
%% commands, abstract environment, Computing Classification System
%% environment and commands, and keywords command.
\maketitle

\section{Introduction}
A compiler translates human-readable source code into executable machine code. In addition, modern 
optimizing production compilers like LLVM apply a broad spectrum of transformations, from local 
rewrites to interprocedural analyses across whole programs. Such optimizations occur throughout the 
compilation pipeline, including instruction selection, where the program is mapped onto the target-specific
instruction set, and are guided by performance heuristics such as runtime, register pressure, and instruction
count. The transformations are expected to preserve  original program semantics, though they lack formal 
guarantees of correctness. As a result, miscompilation bugs, where the compiled code diverges from the
intended semantics of the source program, remain a recurring pathology within compiler infrastructures.
% \begin{figure}[ht]
%     \centering
%     \small
%         {\tiny
%     \begin{listing}[H]{
% define i32 @add_unoptimized() {
% entry:
%   %0 = add i32 2, 3
%   %1 = add i32 %0, 4089
%   ret i32 %2
% }
%         \end{lstlisting}
%         }
%         \caption{Unoptimized function}
%         \label{fig:add_unoptimized}
%     \hfill
% %     \begin{subfigure}[b]{0.48\textwidth}
% %         \centering
% %         \small
% %        {\tiny
% %     \begin{lstlisting}
% % define i32 @add_optimized() {
% % entry:
% %   ret i32 4095
% % }
% %         \end{lstlisting}
% %         }
% %         \caption{Optimized function}
% %         \label{fig:add_optimized}
% %     \end{subfigure}
% %     \caption{Example of compiler optimization via constant folding over addition in LLVM IR. (a) the addition operation is explicit; (b) the addition is replaced with a constant return value.
% %     }
% %     \label{fig:intro_constant_folding}
% \label{fig:intro_semantics}
% \end{figure}
% \begin{listing}[H]
% % We cannot put '{' on a line after % in draftonly mode, as the hack we used to
% % not include the draft section will interpret the listing as normal
% % latex where '%' is a comment and {} need to match, which they will
% % not if only one is commented.
% % \begin{mlir}
% % // This is a comment
% def @foo(%0 : !dialect.type)
%  %def i32 @add_unoptimized() 
%  {
%   % 0 = add i32 2, 3
%   % 1 = add i32 %0, 4089
%   ret i32 %2
%  }
% % {
% %   %a = dialect.op(%0) : !dialect.type // $\color{black}\circled{a}$
% % }
% % def i32 @add_unoptimized() 
% %  {
% %   %0 = add i32 2, 3
% %   %1 = add i32 %0, 4089
% %   ret i32 %2
% %  }
% % \end{mlir}
% \caption{Function Amenable to Peephole Optimization.}
% % \caption{A simple MLIR code example with markers. Markers can also be placed in
% % 	captions and refer to labels, e.g. \circled[lst:example]{a}.}
% \label{lst:example-peephole-opportunity}
% \end{listing}
% \begin{listing}[H]
% % \begin{mlir}
% % \end{mlir}
% \caption{Program text after peephole optimization.}
% \label{lst:example-peephole-optimized}
% \end{listing}
Semantic loss during compilation can be caused by implementation bugs or a lack of formal semantics 
that give precise mathematical definitions for constructs in the source and target languages. 
Without clear formalization, ambiguities emerge around what constitutes correct behavior, giving 
compiler developers implicit freedom to interpret the effect of their optimizations inconsistently. 
For example, if a language specification does not explicitly define the behavior of integer overflow 
e.g., wraparound, clamping, undefined behavior, compilers make different assumptions, leading to inconsistent
and potentially incorrect compilation.
% (\autoref{fig:intro_semantics}). 

To address these issues and to demonstrate the possibility of building trustworthy compilation pipelines,
fully verified compilers that offer formal correctness guarantees, e.g., CompCert, 
a verified compiler for the C language, have been developed. However, as noted in, ``adding optimizations 
is limited by the effort required to verify them,'' hinting towards the blocking
proof burden involved. As a result, verified compilers are hard to extend, slow to adapt, and remain
niche tools.

In contrast, modern production compilers advance rapidly through continuous, fast-paced development 
by their community. A prominent example is LLVM, a widely used open-source compiler 
framework forming the compiler middle- and backend for a variety of programming languages, e.g., C, 
Rust, Lean, Haskell.

Moreover, LLVM embodies modern compiler design principles through its architecture built around an 
intermediate representation (IR), which is a language used to represent a program within the compiler 
toolchain that enables language-agnostic optimizations, and its design for iteratively optimizing a 
program throughout its lifetime.\footnote{By program lifetime we refer to the program at different stages
until executed during runtime, e.g, compile time, link time, code generation} By providing a frontend
that lowers source code to LLVM's IR, compiler developers can reuse LLVM's powerful middle and backends,
making LLVM attractive for building new compilers.

At the same time, a bug in LLVM can compromise the correctness of compiled programs across any language
that relies on its compilation tools. In Lean, a programming language and proof assistant, the LLVM-based
compiler is part of the trusted code base, and its compilation results are used to reason in proofs. 
Therefore, the soundness of these proofs hinges on the trustworthiness of the LLVM-based compiler. Similarly,
Rust relies on the LLVM toolchain \cite{Rustlantis}, hence once compiled, preservation of properties 
implemented at the Rust language level relies on the correctness of LLVM. This makes LLVM a critical 
target for compiler verification using formal methods.

\todo{add a figure aka semanitcs figure }
%   \begin{figure}
% \begin{minipage}{\textwidth}
% \begin{listing}
% % def execute_SHIFTIOP (shamt : BitVec 6) (op : sop) (rs1_val : BitVec 64) : BitVec 64 := match op with
% %       | sop.RISCV_SLLI => (shift_bits_left rs1_val shamt)
% %       | sop.RISCV_SRLI => (shift_bits_right rs1_val shamt)
% %       | sop.RISCV_SRAI => (shift_bits_right_arith (rs1_val) shamt)
% \end{listing}
% \end{minipage}
% \caption{
% Formalization of the RISC-V 64-bit shift-immediate instructions. Each instruction is mapped to its corresponding bit-level operation, providing a precise, unambiguous definition.}
% \label{fig:intro_semantics}
% \end{figure}
At the time of this thesis, verification efforts have targeted the LLVM ecosystem around LLVM IR 
(middle end). However, little efforts have focused on backend passes. Despite the backend being considered a ``highly
optimizing compiler in its own''. Any correctness guarantees established in the middle
end can be invalidated by bugs in the backend. Regehr et al. investigated the ARM
backend and uncovered a multifold of bugs, highlighting that these low-level components are the most
error-prone parts of the compilation pipeline. However, such  verification work as in
relied on SMT solvers, which are automated verification tools implemented by a large, unverified 
codebases with a history of bugs. Nuno et Al. noted ``we did encounter solver bugs while performing 
this work'' in, where they aimed to detect misoptimizations in LLVM’s middle end, but encountered that
20\% of the reported issues were bugs in the SMT solver itself. Moreover, correctness is based on 
handwritten ISA semantics, and the compiler backend's work is verified by first performing an unverified
lift into LLVM IR, then reusing translation validators for the middle end. In contrast, we argue that the
compiler backend should itself be the subject of full formal verification justified by the issues 
encountered in previous work.\newline
\textbf{Our Approach}:
To address these limitations and increase confidence in the work performed by compiler backends, we 
present a prototype of a formally verified instruction selection pipeline for a subset of LLVM IR 
targeting the RISC-V ISA for 64-bit architecture. This system is implemented within the Lean interactive
theorem prover and lowers functions, written in a pure arithmetic fragment of LLVM IR, into a RISC-V
assembly dialect targeting a 64-bit architecture.  When a source program conforms to the supported
IR fragment, our pipeline guarantees that the selected RISC-V instructions behave in accordance 
with the original LLVM IR semantics. This claim is supported by a Lean proof in every case. To realize our project, we investigate how intermediate representations and instruction set 
architectures can be formally modeled in Lean, and, in this context, how formal processor models 
can be used to derive authoritative semantics for the target ISA to reason in a proof assistant. 
These language models within the interactive theorem prover then form the basis for verification and 
implementation of our instruction selection. We explore how to extract and reconstruct LLVM’s 
instruction selection patterns, with the goal of reimplementing and verifying selected components 
in our instruction selector. This effort sheds light on the often opaque internals of production 
compiler backends and demonstrates hands-on verification work in a realistic compiler phase. To 
improve the quality of the generated code, we incorporate semantics-preserving, assembly-level 
peephole optimizations. These eliminate inefficiencies introduced during instruction selection and
are again inspired by ``combiner'', optimization patterns used in the LLVM backends.\newline
Our design emphasizes modularity and extensibility with little proof burden: instruction selection 
patterns are represented as verified rewrite rules, and the associated correctness proofs is 
automatically discharged wherever possible by our implemented proof automation. This stands in 
contrast to verification efforts, which often require  manual proof engineering for each extension. 
For the verification, instead of relying on unverified SMT solvers with large codebases, we formalize
our proofs in an interactive theorem prover with a minimal trusted codebase and robust proof automation
for bit-vector reasoning.\newline
Namely, the Lean theorem prover has recently received a novel bit-vector library and 
supports \texttt{bv\_decide}, the first fully verified SMT solver embedded in a 
theorem prover. This enables automated reasoning over fixed-width bit-vector expressions, 
allowing us to mechanically verify the correctness of rewrite rules and optimization patterns by 
using the \texttt{bv\_decide} bit-blaster.

Together, these components enable a proof of concept for verified instruction selection with 
integrated optimizations, implemented in a theorem prover using a verified bit-blaster to prove LLVM 
instruction selection patterns. We also provide a commandline interface for our instruction 
selection pass, inspired by the LLVM optimizer, that provides an end-to-end pipeline where the 
instruction selection and optimizations at core are verified.



% Of this super cool article \ldots
% \grosser{A simple comment refering to a particular location}

% If there is \authorTwo[some text we want to refer to specifically]{this looks
% particularly interesting} we can refer to this text from within our comments.

% And here we only mark \authorTwo[text]{} that is \authorTwo[interesting]{}, but do not
% actually comment.

% Now we have a sequence of comments,
% \authorThree{Comment 3}
% \authorFour{Comment 4}
% \authorFive{Comment 5}
% \authorSix{Comment 6}
% which should not introduce overlong white space sequences.

% Sometimes it is also important to \emph{emphasize} text to understand if it is
% underlined or printed in italic.

% We sometimes use macros to state that a tool as the name \toolname{} in a
% flexible way.

% \lipsum[1-3]

% \begin{figure}
% Link to figure
%
% https://docs.google.com/drawings/d/1juKp43D3rLC-luBQPwQZ_wCnDK2S_6C1k6USV0wKE0g/edit?usp=sharing
% \includegraphics[width=\columnwidth]{overview.pdf}
% \caption{Our key idea visualized}
% \grosser{Replace this figure with your own drawing.}
% \end{figure}

\vspace{.5em}
\noindent
Our contributions are:\sarah{ref the section where I resolve this contribution}
\begin{itemize}
\item A software representation of the pure RISC-V base ISA and \grosser[selected RISC-V extensions]{be concrete! - can you list the extensions you support?}, implemented Lean-native
 as an SSA-based IR with provable authoritative RISC-V denotational semantics designed amenable for
  proof automation via bit-blasting directly in ITPs (compared to large, complex processor models)
  (\autoref{sec:riscv-ssa}).

\item End-to-end certified library of \grosser[instruction selection and optimization patterns]{Can we make this concrete, e.g., count the number of patterns, or claim completeness on certain subsets of the LLVM backend?} extracted from
 the underexplored LLVM backends, verified against the official RISC-V semantics relying on an extensible,
  generalizabel toolchain we have built in Lean to model MLIR-style hybrid IRs and to verify their progressive
   lowering including MLIR conversion cast (\autoref{sec:inst-selection-automation}).

\item Implementation of an extensible, fully verified instruction selector prototype for LLVM IR built
 atop the extensible, certified pattern-rewrite library with full proof discharge of the refining 
 rewrite patterns and straightforward extension and verification of new rewrites by an end-to-end verified 
 bit-blaster in Lean. Therefore, we are relying solely on dependent-type ITP-native verification tools
  with a minimal trusted codebase, as opposed to unverified SMT solvers.

\item An interactive, end-to-end pipeline prototype directly invokable over LLVM IR files via 
command-line in Lean that lowers  LLVM IR to RISC-V register-allocated assembly, with fully verified,
 scalable instruction selection and optimization at its core  demonstrating that realistic compiler 
 routines can be formally verified and implemented within an ITP.

\item An evaluation demonstrating the use of our pipeline and models in Lean by verifying the LLVM 
components of the previously unverified RISC-V code generation test suite in LLVM. Additionally, we 
show that our verified code generation approach — focusing on instruction selection — achieves performance 
close to that of established compilers by lowering and evaluating \sarah[XY]{} randomly generated programs.
\sarah{to be determined} (\autoref{sec:evaluation}).
\end{itemize}

\section{The Idea: Certified Instruction Selection at Scale}

\grosser{Give a high-level overview of your idea. Let the reader \textbf{feel} how streamlined
the process of defining a small peephole rewriter is. Give a couple of concrete examples.}

\section{Background}
\label{sec:background}


\section{An SSA-based RISC-V IR Certified Against the Authoritative RISC-V Semantics}
\label{sec:riscv-ssa}

\section{Using Bit-Balasting to Certify Instruction Selection Rewrite Automatically}
\label{sec:inst-selection-automation}

\section{Implementation}
\label{sec:implementation}

\section{Evaluation}
\label{sec:evaluation}

\section{Related Work}

\section{Conclusion}

%% Acknowledgments
\begin{acks}                            %% acks environment is optional
                                        %% contents suppressed with 'anonymous'
  %% Commands \grantsponsor{<sponsorID>}{<name>}{<url>} and
  %% \grantnum[<url>]{<sponsorID>}{<number>} should be used to
  %% acknowledge financial support and will be used by metadata
  %% extraction tools.
  This material is based upon work supported by the
  \grantsponsor{GS100000001}{National Science
    Foundation}{http://dx.doi.org/10.13039/100000001} under Grant
  No.~\grantnum{GS100000001}{nnnnnnn} and Grant
  No.~\grantnum{GS100000001}{mmmmmmm}.  Any opinions, findings, and
  conclusions or recommendations expressed in this material are those
  of the author and do not necessarily reflect the views of the
  National Science Foundation.
\end{acks}

%% Bibliography
\bibliography{references}


%% Appendix
% Move \cleardouble and \appendix out of `draftonly` if you are using the
% appendix
\begin{draftonly}
\cleardoublepage
\appendix
\section{Formatting and Writing Guidelines}

These formatting guidelines aim to standardize our writing. They ensure that
papers with multiple authors have a consistent look and that commonly occurring
items are formatted in ways that are known to work well.

\subsection{Figures}
\label{appendix:figures}

\paragraph{Referencing Figures} When referencing figures from the text we
ensure the following:
\begin{description}
      \item [All figures are referenced] A paper with un-referenced figures appears incomplete.
        We can check this using the \texttt{refcheck} package.
        Issuing \texttt{make refcheck} on the commandline lists all \emph{labeled} elements that are not referenced in the text.
      \item [References to figures are brief and easy to skip]~\\
                We minimize the number of words needed to refer to a figure. Reducing
                the number of non-information-carrying words directly increases
		the density of interesting content. When skipping references
		becomes easy, reading quickly while ignoring figures remains a
		smooth experience. The best and briefest reference to a figure
		is a link in parenthesis that is added after the subject
		representing the content depicted in a figure:\\
		{\color{pairedTwoDarkBlue}\textit{Figure
		X shows the design of A, which consists of ...}}\\
		$\to$ {\color{pairedFourDarkGreen}
		\textit{The design of A (Figure X) consists of ...}}
      \item [The text is always self-contained without figures] ~\\ The reader
                should be able to read the text without ever looking at any
                figure. They should still understand the text and get the key
		message of each figure directly from the text. By not forcing
		the reader to analyze a figure while reading, we increase
		readability as the reader can continue reading without having
		to skip between text and figures. Such writing style also helps
		to guide the thoughts of the reader, who can (for a moment)
		trust our summary of the figure and does not need to develop
		their own interpretation on-the-fly, a task which often yields
		results that do not fit the flow of our exposition. Readers
		typically only feel that their reading is interrupted if there
		is no explanation of a figure at all. Hence, we do not need to
		discuss all details of a figure, but half a sentence that explains the
		core idea is typically sufficient for a reader to continue
		reading.  By making
		our text self-contained even when ignoring figures the reader
		experiences a smooth and uninterrupted reading experience.\\
		{\color{pairedTwoDarkBlue}
		\textit{The speedups are presented in Figure X. < a new topic> }}\\
		$\to$ {\color{pairedFourDarkGreen}\textit{Our approach outperforms the state of the art
		XXX-library (Figure 3) demonstrating more than 4x speedup on
		test case 1 and 2 and a geometric mean speedup of 1.5x over all
		20 test cases.}}
\end{description}

We reference figures in text using
\texttt{\symbol{92}autoref\{fig:speedup\}} for a figure with label
\texttt{fig:speedup}.  The use of autoref ensures that all references
to figures are formatted consistently, e.g. as \autoref{fig:speedup}.

\paragraph{Color Scheme}

In Figures we use a color scheme that is print-friendly and also visible
with red-green blindness. The following colors are all print-friendly
and red-green save when only using Color 1-4:

\medskip
{
	\small
\newcolumntype{a}{>{\columncolor{pairedOneLightBlue}}c}
\newcolumntype{b}{>{\columncolor{pairedTwoDarkBlue}}c}
\newcolumntype{d}{>{\columncolor{pairedThreeLightGreen}}c}
\newcolumntype{e}{>{\columncolor{pairedFourDarkGreen}}c}
\newcolumntype{f}{>{\columncolor{pairedFiveLightRed}}c}
\newcolumntype{g}{>{\columncolor{pairedSixDarkRed}}c}

\begin{tabular}{a b d e f g}
Color 1 & Color 2 & Color 3 & Color 4 & Color 5 & Color 6\\
\#a6cee3 & \#1f78b4 & \#b2df8a & \#33a02c & \#fb9a99 & \#e31a1c
\end{tabular}
}

We de-emphasize components in figures by using additionally two shades of gray.
Especially in complex figures, it is often helpful to de-emphasize visual
elements that we want to represent but that should not be the focus of a
reader's attention.

\medskip
{
	\small
\newcolumntype{h}{>{\columncolor{pairedNegOneLightGray}}c}
\newcolumntype{i}{>{\columncolor{pairedNegTwoDarkGray}}c}

\begin{tabular}{h i}
Color -1 & Color -2\\
\#cacaca & \#827b7b\\
\end{tabular}
}

Single-color graphs are plotted in Color 1 - Light Blue.

\paragraph{Labels in Figures}
Complex diagrams often benefit from labels inside the diagrams. We suggest to
use a filled circle (e.g, in light blue) to highlight these numbers and use
these references, e.g., \circled{1} implemented as \texttt{\textbackslash{}circled\{1\}}, in the text to refer to them.

\paragraph{Captions and Core Message}
\label{appendix:captions}

Each figure should have a caption that makes a clear statement about this
figure, as such a statement makes it easier for the reader to (in)validate the
figure as evidence for the claim we make. Traditionally, figures often have a
caption indicating its content:\\ {\color{pairedTwoDarkBlue} \textit{$\cdot$
Speedup of approach A vs approach B on system X}}\\ {\color{pairedTwoDarkBlue}
\textit{$\cdot$ Architecture diagram of our solution}}\\ While these statements
clearly state the content of a figure at the meta-level, they often lack
information about the precise content and the claim a figure is meant to
evidence. While knowing that a figure is an architecture diagram is useful for
the reader when looking at the figure the reader automatically asks two
questions: (a) what properties set this architecture apart and (b) does its
implementation deliver the claimed properties? Or, in more general terms, what
claims do we aim to evidence with this figure and does the figure provide the
needed evidence to support our claims? In theory, this information could be
contained in the text of the paper, but to optimize for readers who skim the
figures first, we want to offer them as part of the caption. Nevertheless, it
makes often sense to word the caption strategically to still document the
meta-level content of a figure.\\ $\to$ {\color{pairedFourDarkGreen}
\textit{$\cdot$ Approach A is consistently faster than approach B, except for
inputs that are not used in practice}}\\ $\to$ {\color{pairedFourDarkGreen}
\textit{$\cdot$ The architecture of our design increases reusability by making
components A, B, \& C independent of the core.}} For example, after reading the
last caption the reader can validate if the architecture design indeed enables
the promised independence, and we can double-check while drafting the paper that
our figure is visualized to facilitate checking if it works as evidence. ! This
does not mean we should mislead with our figure but rather make things easy to
check. If our figure or data would not support our claim, it should be similarly
easy to invalidate our claim!

\subsubsection{Plots} We use matplotlib to create performance
plots such as \autoref{fig:speedup}. We use the following
formatting guidelines:
\begin{itemize}
  \item Use a vertical y-label to make it easier to read.
  \item Remove top and right frames to reduce visual noise
	and allow the reader to focus on the data in the
	figure.
  \item Provide the concrete data at the top of each bar.
\end{itemize}

\noindent
We also suggest to follow these technical remarks:
\begin{itemize}
  \item Create pdf plots and do not use bitmap formats (e.g., png) to
	ensure high quality when zooming in.
  \item Avoid Type-3 bitmap fonts by
	setting fonttype to 42.
\end{itemize}

\begin{figure}
\includegraphics[width=\columnwidth]{plots/speedup}
\caption{Improved running speed after 4 weeks of training.
}
\label{fig:speedup}
\end{figure}

\subsubsection{Tables} We optimize our tables for readability by removing as
much clutter as possible, while highlighting the key structure. Markus Püschel
(see doc/paper-writing/guide-tables.pdf) wrote a nice guide on how to make nice
tables. \autoref{tab:simple_table} illustrates this with a simple
table.

\begin{table}
\ra{1.2}
\centering
\begin{tabular}{l l l r}
  \toprule
  \textbf{Animal} & \textbf{Size} & \textbf{Biotope} & \textbf{Age}\\
  \midrule
  Dog & Medium & Ground & 20\\
  Cat & Medium & Ground &20 \\
  Ant & Small & Ground & 30 \\
  Elephant & Large & Ground & 70\\
  Whale & Large & Water & 100\\
  Salmon & Medium & Water & 13 \\
  Eagle & Large & Air & 35 \\
  \bottomrule
\end{tabular}
\vspace{1em}
\caption{A table with heigh lines and emphasized header.}
\label{tab:simple_table}
\end{table}

\subsubsection{Listings} We aim to use minted to create listings as much as
possible, as this allows us to edit code quickly. We use syntax highlighting
to make the parts of the code that matter most stand out. Hence, we keep
most code black, comments gray, and highlight just the MLIR operands that
we care about most.

\begin{listing}[H]
% We cannot put '{' on a line after % in draftonly mode, as the hack we used to
% not include the draft section will interpret the listing as normal
% latex where '%' is a comment and {} need to match, which they will
% not if only one is commented.
% \begin{mlir}
% // This is a comment
% def @foo(%0 : !dialect.type)
% {
%   %a = dialect.op(%0) : !dialect.type // $\color{black}\circled{a}$
% }
% \end{mlir}
\caption{A simple MLIR code example with markers. Markers can also be placed in
	captions and refer to labels, e.g. \circled[lst:example]{a}.}
\label{lst:example}
\end{listing}

\begin{listing}[H]
% \begin{lean4}
% theorem funext {f₁ f₂ : ∀ (x : α), β x}
%   (h : ∀ x, f₁ x = f₂ x) : f₁ = f₂ := by
%   show extfunApp (Quotient.mk' f₁) = 
%        extfunApp (Quotient.mk' f₂)
%   apply congrArg
%   apply Quotient.sound
%   exact h
% \end{lean4}
\caption{A simple Lean4 code example, taken from
  \url{https://lean-lang.org/lean4/doc/syntax\_highlight\_in\_latex.html\#example-with-minted}.}
\end{listing}

The syntax highlighting also works for xDSL-like IRs.
Notice that different minted styles can be used for different environments.
The xDSL environment uses the murphy-style in this case, whereas the MLIR version applies the colorful-style.

% \begin{xdsl*}{fontsize=\scriptsize}
% func.func() [sym_name = "main", function_type = !fun<[
%               !iterators.columnar_batch<!tuple<[!i64]>>
%                  ], []>] {
%   ^bb0(%0 : !iterators.columnar_batch<!tuple<[!i64]>>):
%     %t : !iterators.stream<!llvm.struct<[!i64]>> =
%       iterators.scan_columnar_batch(%0 : ...)
%     %filtered : !iterators.stream<!llvm.struct<[!i64]>> =
%           iterators.filter(%input : …) [predicateRef = @s0]
%     iterators.sink(%filtered : !iterators.stream<!tuple<[!i64]>>)
%     func.return()
% }

% func.func() [sym_name = "s0", function_type = !fun<[
%     !llvm.struct<[!i64]>], [!i1]>] {
%   ^bb0(%struct : !llvm.struct<[!i64]>):
%     %id : !i64 = llvm.extractvalue(%struct : ...)
%               [position = [0 : !index]]
%     %five  : !i64 = arith.constant() [value = 5 : !i64]
%     %cmp : !i1 = arith.cmpi(%id : !i64, %five : !i64)
%               [predicate = 4 : !i64]
%     func.return(%cmp : !i1)
% }
% \end{xdsl*}

The code in this document was compiled with minted version: \csname ver@minted.sty\endcsname.

\section{Writing}

A couple of hints with respect to how we write text.

\subsection{Citations}
\label{appendix:citations}

\subsubsection{Do not use numerical citations as nouns}
Especially when working with numerical citations (e.g., [1]) the use of
citations as nouns reduces readability. Hence, we do not use numerical citations
as nouns and instead expand these citations with \texttt{\textbackslash{}citet} to the
authornames.
\\
{\color{pairedTwoDarkBlue}
\textit{[1] showed that .. $\dots$}}\\
$\to$ {\color{pairedFourDarkGreen}\textit{Author et al. [1] showed}}

\subsubsection{Prefer meaningful text over citations as textual content}
While acknowledging authors of work is important, maximizing the amount of technical
content (outside of a historic perspective) typically makes text more direct and
concrete. Hence, we avoid the discussion of who did what in text if the
historic context does not add meaning or empty words can be replaced by an immediate
citation. E.g, in the following the words `introduced in` are not carrying
information and can be dropped.\\
{\color{pairedTwoDarkBlue}
\textit{we extend PreviousIdea introduced in [1] $\dots$ by}}\\
$\to$ {\color{pairedFourDarkGreen}\textit{we extend PreviousIdea [1] by}}


\subsubsection{Managing acronyms automatically}
Managing acronyms manually can lead to situations where the specific term is not properly expanded upon first use or when it is introduced.
The \texttt{acronym} package is useful to avoid such situations and provides full control over acronyms.
The expanded form of an abbreviation should be in lowercase, unless its parts are also capitalized (e.g., United Kingdom for UK).
For example, assume we have defined an acronym with \texttt{\textbackslash{}newacronym\{ir\}\{IR\}\{intermediate representation\}}:
\begin{itemize}
  \item Upon first use of \texttt{\textbackslash{}ac\{ir\}} we get: \ac{ir}.
  \item On the second reference: \ac{ir}.
  \item To force expansion (e.g., for the background section where the term is first described), we use \texttt{\textbackslash{}acf\{ir\}} which gives: \acf{ir}.
  \item To force contraction (e.g., to save space for a figure caption), we use \texttt{\textbackslash{}acs\{ir\}} which gives: \acs{ir}.
  \item To obtain plural form, we use \texttt{\textbackslash{}acp\{ir\}} giving: \acp{ir}.
\end{itemize}

\subsubsection{Adding hyphenation rules}
While \LaTeX\ handles word breaks automatically, and packages like \texttt{microtype} aim to minimize word splitting, there are instances where either new words lack hyphenation rules, or the suggested hyphenation for a word is undesirable.
The \texttt{hyphenat} package allows adding hyphenation rules using the \texttt{\textbackslash{}hyphenation} macro, e.g., \texttt{\textbackslash{}hyphenation\{Alex-Net\}} for AlexNet.

Allowing hyphenation of compound words, we can use \texttt{\textbackslash{}-/} from the \texttt{extdash} package, for example \texttt{high\-/level} can be written as \texttt{high\textbackslash{}-/level}.
Disallowing a line break at the compound word hyphen, we can use \texttt{\textbackslash{}=/}, as \texttt{RISC\textbackslash{}=/V} for \texttt{RISC\=/V}.

\end{draftonly}


\end{document}
